% $Name:  $
% $Id: thesis.tex,v 1.18 2010/10/08 13:39:40 paalanen Exp $


% The history of this template:
% - unknown, original version
% - Jarmo "trewas" Ilonen, masters thesis, 2003
% - Pekka "PQ" Paalanen, information processing project, 2004,
%     hints about graphicx and making PDF from Pasi Valminen
% - Pekka "PQ" Paalanen, Master's thesis, 2006
% - upgraded to pdflatex and 1.8.2010 thesis guidelines, 2010

% useful links:
% http://www.ctan.org/tex-archive/help/Catalogue/entries/grfguide.html
% http://www.tug.org/applications/hyperref/


\documentclass{lutmscthesis}[2010/09/22]
%\documentclass[draft]{lutmscthesis}   % leave figures blank, faster

\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english,finnish]{babel}

\usepackage{times}

\usepackage{setspace}
\usepackage{verbatim}
\usepackage[intlimits]{amsmath}

% Ensure figure captions are below and table captions are above the content.
\usepackage{float}
\floatstyle{plain}\restylefloat{figure}
\floatstyle{plaintop}\restylefloat{table}

\usepackage[pdfborder={0 0 0}]{hyperref}
%\usepackage[chapter]{algorithm}

%           Hyperref rationale - or just pain in the butt
%
% Load 'float' package first, because that will fix problems with 'algorithm'
% package interacting with hyperref.
%
% Hyperref must be the last package loaded, except...
% Load 'algorithm' AFTER hyperref, otherwise \theHalgorithm is
% undefined control sequence error appears.
%
% The TeXLive 2008 version of 'algorithmic' is buggy with hyperref.
% Use this bundled, special, hand-fixed version of algorithmic.sty
% instead. It is identified by version 2006/12/15.


\graphicspath{{resources/}}                % Graphics search path

\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\boldsymbol{#1}}
\newcommand{\diag}[1]{\mathrm{diag}(#1)}
\newcommand{\iprod}[1]{\left\langle #1 \right\rangle}
\newcommand{\me}{\mathrm{e}}
\newcommand{\mi}{\mathrm{i}}
\newcommand{\md}{\mathrm{d}}
\newcommand{\sse}{{}} %\mathrm{SSE}}
\newcommand{\trace}{\mathrm{Tr}\:}
\newcommand{\frp}[2]{{}^\mathrm{#1}\vect{#2}}
\newcommand{\frs}[3]{{}^\mathrm{#1}#2_\mathrm{#3}}
\newcommand{\frv}[3]{{}^\mathrm{#1}\vect{#2}_\mathrm{#3}}
\newcommand{\frm}[3]{{}^\mathrm{#1}\matr{#2}_\mathrm{#3}}
\newcommand{\colvec}[2]{\genfrac{[}{]}{0pt}{1}{#1}{#2}}
\newcommand{\relphantom}[1]{\mathrel{\phantom{#1}}}

% Thesis information
\title{Sensitivity of retinal image segmentation on ground truth accuracy}
\titlefin{Silmänpohjakuvien segmentoinnin herkkyys pohjamerkintöjen tarkkuudelle}
\author{Teemu Huovinen}

\Major{Degree Program in Computer Science}
\Majorfin{Tietotekniikan koulutusohjelma}

\Keywords{eye fundus, image segmentation, sensitivity analysis, ground truth}

\Keywordsfin{silmänpohja, segmentointi, herkkyysanalyysi, pohjamerkintä}

% For a single supervisor, \Supervisor{N.N.}
% For multiple supervisors, \Supervisors{N.N.\\ K.K.}, that is,
% use \\ to separate names.
% the same with \Examiner{} or \Examiners{}
\Supervisor{Lasse Lensu D.Sc. (Tech.)}
\Examiners{Lasse Lensu D.Sc. (Tech.)}

% The examiners for the Finnish abstract only.
\Tarkastajat{TkT Lasse Lensu}

% date of topic accepted in the council
\AcceptDate{January 1\textsuperscript{st}, 2014}
% date of signature
\SignDate{July 3\textsuperscript{rd}}
% Year in abstract pages
\Year{2014}

% Thesis statistics: figure, table and appendix counts, for abstracts
\addtostats{, 13 figures, 1 table, and 2 appendices}
\addtostatsfin{, 13 kuvaa, 1 taulukko ja 2 liitettä}

\begin{document}
\selectlanguage{english}

\maketitle
\newpage

\begin{abstract}
% Notice that the page, figure, table and appendix counts are for the
% whole work, including title page, appendices, figures in appendices, etc.
% You have to count the numbers yourself, except for pages.
% %
% Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod
% tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,
% quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
% consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
% cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non
% proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
\end{abstract}


\begin{tiivis}
% Tähän kirjoitetaan ytimekäs tiivistelmä: tausta, tavoite, tulokset ja johtopäätökset.
% Tiivistelmässä kannattaa käyttää lyhyen nasevia lauseita. Itse tekstissä voi käyttää
% monimutkaisempia lauseita. Tiivistelmä-sivu on yksi sivu ja tiivistelmäteksti on yksi
% kappale, ei useita kappaleita. On hyvä kertoa työn tavoitteet. Mikäli työ sisältää oleellisia
% aiheen rajauksia, ne kannattaa mainita jo tiivistelmässä. Työn tulokset ja johtopäätökset
% luetellaan lukijan mielenkiinnon lisäämiseksi. Opinnäytetyö kirjoitetaan passiivissa tyyliin
% ``tässä työssä tutkitaan..'' aktiivin sijaan ``minä tutkin...'' ja marginaalit tasataan aina sekä
% vasemmalle että oikealle. Työn numerointi aloitetaan kansilehdeltä, mutta tätä roomalaista
% numeroa ei merkitä näkyviin. 
\end{tiivis}


\begin{preface}

This thesis was done as a part of my computer science degree in Lappeenranta University of Technology,
and as a report for my summer internship at the Laboratory of Machine Vision and Pattern Recognition.
I would like to thank my supervisor Lasse Lensu for giving me this opportunity and for his extensive support
and guidance.

\

Lappeenranta, October 19th, 2014
\end{preface}


% These name-definitions must be after Babel langugage change
% commands, as they redefine these.
\renewcommand\refname{REFERENCES}
\renewcommand\contentsname{CONTENTS}

\pagestyle{masters}
\newpage

\tableofcontents



\section*{ABBREVIATIONS AND SYMBOLS}

\begin{tabular}{l l}
\textbf{RGB} & Red Green Blue\\
\textbf{TP} & True Positive\\
\textbf{TN} & True Negative\\
\textbf{FP} & False Positive\\
\textbf{FN} & False Negative\\
\textbf{FPR} & False Positive Rate\\
\textbf{FNR} & False Negative Rate\\
\textbf{CLAHE} & Contrast Limited Adaptive Histogram Equalization\\
\end{tabular}


% space between paragraphs
\setlength{\parskip}{3ex}


\section{INTRODUCTION}

\subsection{Background}

%Diabetes has established itself as a seemingly permanent, worldwide global problem. 
The growing amount of diabetes patients and (arguably) more importantly the estimated amount of undiagnosed patients 
motivate the research for an effective mass screening method for monitoring and early detection of diabetes. Diabetic retinopathy
is a complication of diabetes that causes abnormalities in the eye, and detecting these abnormalities in the eye fundus
is a promising mass screening method. Images where ophthalmologists have marked these abnormalities, such as
exudates, are used as ground truths in eye fundus image segmentation research.

Optimal ground truth would be a pixel-accurate binary representation of the abnormalities, but as ground truths are
done by a human hand, such accuracy is unrealistic. Because the marking of an accurate ground truth takes a good
amount of time and patience, we often have to settle for rough markings of the present abnormalities. Clusters of
exudates are circled, rather than each small finding specified separately.

\subsection{Objectives and Restrictions}

The objective of this thesis is to evaluate how big of an impact inaccurate ground truth has on various image features
and segmentation methods. Structure of the eye fundus is briefly explored for context.

Ground truth accuracy is explored only from the perspective of exudate detection, and Bristol database is used as it has
accurate ground truths of exudates. Blood vessel detection is explored only to create a mask for them. A rough method
for optic disk detection is also implemented as a preprocessing step for masking reasons.

Both supervised and unsupervised segmentation methods are used. In supervised methods, ground truths are used to label
observations as either exudate or background. In unsupervised methods, ground truth is used to evaluate segmentation
results. Best parameters for each method are chosen based on their performance.

\subsection{Structure of the Thesis}

Section~\ref{sec:segmentation} takes a look at the different features
of eye fundus images, and how they are relevant in this thesis.
It also explains the theory behind the applied pre-processing
and segmentation methods.
Section~\ref{sec:sensitivity} details how sensitivity analysis is 
done in this thesis, and also explains the used evaluation methods.
Section~\ref{sec:experiments} describes the experiments in detail,
and presents the results for each experiment.
% Section~\ref{sec:discussion} sums up and interprets the results, and
% discusses the impact of this thesis and possible future work this thesis
% might invoke.

\section{RETINAL IMAGES AND THEIR SEGMENTATION}
\label{sec:segmentation}

\subsection{Structure of the eye fundus}
The function of the human eye on an abstract level can be compared to a camera. Light reflected from an object passes through the cornea,
pupil and lens and is focused on the retina (the inner part of the eye). It is then processed by light sensitive
cells and then transmitted to the brain via the optic nerve. Rather than covering the eye as a whole, this chapter will focus on
the structure of the eye fundus.

The eye fundus is the area located in the retina, the interior wall of the ''backside'' of the eye. Its
most noticeable parts are the optic disk, the macula (and fovea), and the arteries. Macula is a highly light sensitive area
in the central region of the retina. Fovea is a round area located in the macula, where almost all the cells sensitive to
color and fine detail (called cones) are found.

Optic disc is where the optic nerve and main arteries connect with the eye. There are no light sensitive cells inside the
optic disc, which creates a blind spot in the retina. Arteries inside the eye together with the choroid (vascular layer
surrounding the retina) provide nutritional support to the eye~\cite{Kauppi:2010}. These parts of the eye fundus
are illustrated in Figure~\ref{fig:healthyeyefundus}.

\begin{figure}[ht]
\centering 
{ \includegraphics[width=0.7\textwidth]{healthyeyefundus} }    
  \caption[moving]{Structure of the eye fundus
  \label{fig:healthyeyefundus}}
\end{figure}


\subsection{Optic disc detection}

Optic disc is very similar to exudates in terms of color and intensity, so detection
and masking of the optic disk is an important preprocessing step in exudate detection.
There are papers dedicated to the localisation of the optic disk~\cite{Sekhar:2008},
and it is also covered in papers concerning the detection of other parts of the eye
fundus, such as exudates~\cite{Walter:2002}.

This method is based on the brightness of the optic disk, and the vertical blood vessels inside it.
The horizontal image gradient is calculated using Sobel gradient operator, the result is shown in 
Figure~\ref{fig:opticdisk_gradient}. Image is then divided into slightly overlapping square 
areas with a side of 140 pixels (size is adjusted when operating close to image borders). The area
with the highest sum of gradients is considered as region of interest, i.e. to hold the optic disk.
This is because the dark blood vessels inside the bright optic disk result in a strong horizontal
gradient. Images with a ``camera glare``,i.e. a high intensity strip in the corner of the eye fundus
are problematic, as that area also has a high horizontal gradient. Region of interest is shown in Figure~\ref{fig:opticdisk_ROI}.

Inside this area of highest sum of horizontal gradients, the pixel with the highest intensity is considered
to be inside the optic disk. This pixel is then used as a center of a circle that will mask out the optic disk.
Final masking result is shown in Figure~\ref{fig:opticdisk_masked}.

\begin{figure}[ht]
\centering
{
  \subfloat[]{
     \label{fig:opticdisk_gradient}
     \includegraphics[width=0.45\textwidth]{OP_gradient}}
  \subfloat[]{
     \label{fig:opticdisk_ROI}
     \includegraphics[width=0.45\textwidth]{OP_ROI}}
     
     
  \subfloat[]{
     \label{fig:opticdisk_masked}
     \includegraphics[width=0.45\textwidth]{OP_masked}}

  \caption[moving]{Locating and masking the optic disk:
  \subref{fig:opticdisk_gradient} Horizontal gradient
  \subref{fig:opticdisk_ROI} Region of interest
  \subref{fig:opticdisk_masked} Optic disk masked out}
  \label{fig:opticdisk}
}\end{figure}


\subsection{Blood vessel detection}
\label{subsec:vesselmask}

The purpose of blood vessel detection in this thesis is to create a mask, and to use that mask to remove false
positives from exudate segmentation results. For example, edge detection techniques often highlight the
borders of vessels as well as exudates.

The mask is formed by first using contrast limited adaptive histogram equalization (CLAHE)~\cite{Reza:2004} to enhance
contrast in the green channel of the image, the result for this is shown in Figure~\ref{fig:vesselmask_adaptive}. This
contrast enhanced image is then thresholded with Otsu's method~\cite{Otsu:1975}, which separates the image into foreground
and background by minimizing the intra-class variance. This results in all the vessels and other darker areas showing as
black (or background), and all brighter areas as white (foreground). This is shown in Figure~\ref{fig:vesselmask_threshold}.
To create a binary mask of the darker areas, we use the complement of this thresholded image. Final version of the
mask is shown in Figure~\ref{fig:vesselmask_complement}.

This method is inadequate for blood vessel detection as it also includes other darker areas of the image, 
such as the fovea. As a mask however, it clearly reduces the amount of false positives in exudate segmentation results.
It also doesn't remove true positives, as only the darker areas of the image are included in the mask.

\begin{figure}[ht]
\centering
{
  \subfloat[]{
     \label{fig:vesselmask_adaptive}
     \includegraphics[width=0.45\textwidth]{vesselmask_adaptive}}
  \subfloat[]{
     \label{fig:vesselmask_threshold}
     \includegraphics[width=0.45\textwidth]{vesselmask_threshold}}
     
     
  \subfloat[]{
     \label{fig:vesselmask_complement}
     \includegraphics[width=0.45\textwidth]{vesselmask_complement}}

  \caption[moving]{Phases in creating the blood vessel mask:
  \subref{fig:vesselmask_adaptive} CLAHE
  \subref{fig:vesselmask_threshold} Thresholding using Otsu's method
  \subref{fig:vesselmask_complement} Final mask, complement of thresholded image}
  \label{fig:vesselmask}
}\end{figure}

\subsection{Color transform}

The human eye is capable of correcting the effect of varying light sources (illuminants) in perception of color. This is why
a red chair will look (more or less) red in the sun and in the moonlight. In contrast, an image of an object taken under different
light sources is perceived differently in terms of color by a computer. This is why color features need to be normalized in image
sets where there's high variation in color properties.

The illumination and general color properties of the eye fundus images in Bristol database vary quite heavily.
To be able to effectively use color features in teaching a classifier, the variance between image needs to be minimized.
This is achieved by estimating the illuminant by applying the gray-world assumption~\cite{Buchsbaum:1980}:
\textit{the average reflectance in a scene under a neutral light source is achromatic.} Color feature normalization was 
then done by multiplying each channel with a coefficient defined as $ K_r = I_{avg} / r_{avg}$, where $I_{avg}$ is the mean
of all RGB-values in the image, and $r_{avg}$ is the mean of the specific channel. Results are shown in Figure~\ref{fig:colortransform}.

\begin{figure}[ht]
\centering
{
  \subfloat[]{
     \label{fig:original_010}
     \includegraphics[width=0.45\textwidth]{original_010}}     
  \subfloat[]{
     \label{fig:color_010}
     \includegraphics[width=0.45\textwidth]{color_010}}
     
  \subfloat[]{
     \label{fig:original_011}
     \includegraphics[width=0.45\textwidth]{original_011}}     
  \subfloat[]{
     \label{fig:color_011}
     \includegraphics[width=0.45\textwidth]{color_011}}

  \caption[moving]{Color space transform: Original images on the left, adjusted images on the right.}  
  \label{fig:colortransform}
}\end{figure}


\subsection{Unsupervised methods}

An unsupervised method doesn't hold any information of the features it seeks, nor does it have any ''expectations'' of the image.
It perform the same operations on each image with the object of clustering the data into meaningful sets.

Most edge detection methods are unsupervised algorithms. Edge is defined as a sharp change in image and color intensity, 
and the purpose of an edge detection algorithm is to emphasize and detect those sharp changes in an image. In an eye fundus image,
this would include the exudates, but also the borders of the blood vessels and optic disc. As such, edge detection isn't
sufficient for exudate detection since these unwanted regions need to be subtracted from the initial result. During this study,
this is accomplished with the mask acquired in Section~\ref{subsec:vesselmask}.

% Edge: Kirsch Tophat Mehdi

% Jaafar (kmeans?) ravishankar sauvola




\subsection{Supervised methods}

% K-Nearest Neighbor /Naive-Bayes / Gaussian Mixture Model Bayes

Before a supervised method (or its classifier) can be used in classification, it needs to be trained. A specific training set
of labeled images is used to teach the classifier. The method then extracts features from the training images, and generally forms a model
for each used label. For example, to describe a fruit you might use its color and shape. You ''teach'' the method what each
fruit looks like by labeling a set of training images. The classifier learns which features describe each label. Yellow and round equals orange,
yellow and long equals banana, green and round equals apple. After training, the classifier can be used to classify new images.





\section{SENSITIVITY ANALYSIS OF IMAGE FEATURES}
\label{sec:sensitivity}

\subsection{Evaluation methods}

In evaluation, correctly classified samples can be split into true positives and true negatives. Errors are then similarly split
into false positives and false negatives. In this thesis, true positives are findings correctly classified as exudates 
in the segmentation results, and true negatives are consequently findings correctly classified as background. False
negatives are classified as background in results, but are actually positive (classified as exudate) in ground truth, and
false positives are classified as positive in results, when they are in fact background~\cite{Fawcett:2006}. %These terms are illustrated with
% a error matrix shown in Table ~\ref{tab:truepositive}.
% 
% \begin{table}[hpt]
% \begin{center}
% \caption{General error matrix\label{tab:truepositive}}
% 
% \begin{tabular}{cc|c|c|c|c|l}
% \cline{3-4}
% & & \multicolumn{2}{ c| }{Ground truth} \\ \cline{3-4}
% & & Positive & Negative  \\ \cline{1-4}
% \multicolumn{1}{ |c| }{\multirow{}{}{Tests} } &
% \multicolumn{1}{ |c| }{Positive} & True Positive (TP)& False Positive (FP) \\ \cline{2-4}
% \multicolumn{1}{ |c  }{}  &
% \multicolumn{1}{ |c| }{Negative} & False Negative (FN)& True Negative (TN) \\ \cline{1-4}
% 
% \end{tabular}
% \end{center}
% \end{table}


There exists a multitude of ways to evaluate segmentation results. In this thesis, it was desired to have a single number
describing the ''goodness'' of segmentation results to enable easier comparison and ranking of results. Different values,
such as sensitivity, specificity and precision (defined in~\cite{Fawcett:2006}) were considered, but these were ignored as they
tended to only describe one aspect of the result, either the amount of samples correctly positive or negative. The used coefficient
would have to describe the ''goodness'' in a more wholesome way, and for that reason, the Dice coefficient, Jaccard index    
and F-score were considered. These coefficients are defined as follows:

\begin{equation}\label{eq:dice}
\begin{split}
Dice = \frac{2|A\cap B|}{|A| + |B|}
\end{split}
\end{equation}

\begin{equation}\label{eq:jaccard}
\begin{split}
Jaccard = \frac{|A\cap B|}{|A\cup B|}
\end{split}
\end{equation}

\begin{equation}\label{eq:fscore}
\begin{split}
F-score = \frac{2PR}{P + R}
\end{split}
\end{equation}


where A is the segmented set, and B is the ground truth, P stands for precision and R stands for recall (sensitivity).
%Dice coefficient can also be described using true positives, true negatives, false positives and false negatives:

%\begin{equation}\label{eq:diceTn}
%\begin{split}
%Dice = \frac{2TP}{(FP+TP)+(TP +FN)}
%\end{split}
%\end{equation}

Dice coefficient and F-score values are interchangeable in practice as their values are exactly equal, and Jaccard index behaves 
similary to the two. Dice coefficient was chosen as it is very simple to implement and understand. It can have values from the
range [0,1], where 0 indicates no similarities, and 1 indicates perfect agreement.



\subsection{Effects of inaccurate ground truth}

When researching feature sensitivity to ground truth accuracy, standard segmentation with unsupervised methods isn't really useful as
it doesn't use the ground truth. In this thesis, the ground truth is used in finding the best parameters for unsupervised methods.
First, the images used for teaching are segmented with each method and a large set of parameters. The results are then evaluated with
the ground truth. Parameters with the highest Dice (see Eq.~\ref{eq:dice}) coefficient are then selected to be used in experiments.

The accuracy of ground truth has a direct impact on training and performance of supervised methods. In general, inaccurate ground truth 
will mean a significant amount of background samples close to exudates will be categorized as positive. This will result in higher 
variation of feature distributions and an increase of false positives.

\section{EXPERIMENTS AND RESULTS}
\label{sec:experiments}
% \subsection{Retinal Image Databases}

\subsection{The Ground Truth}

The ground truth of exudates in Bristol database is very accurate, and to enable comparison of results and sensitivity analysis,
more inaccurate ground truths were made by hand. Instead of the original color images, markings for the inaccurate ground truth were
made on the black-and-white images of Bristol ground truth. This was to ensure every exudate present in the Bristol ground truth was
also present in the inaccurate ground truth. Markings were done in a way that estimated the way doctors marked their findings 
when given the freedom to make inaccurate markings. Essentially this means that clusters of exudates are grouped together,
and single exudates were more loosely circled. This is illustrated in Figure~\ref{fig:groundtruth}.

To create a basis for more comprehensive testing, different stages of inaccurate ground truths were created by dilating the accurate
ground truth. Three different stages were created; one, three and five iterations of dilation by a disk-shaped structuring element, with a radius of 1 pixel.
The dilation was restricted to stay inside the inaccurate ground truth.

\begin{figure}[ht]
\centering
{
  \subfloat[]{
     \label{fig:bristol_gt}
     \includegraphics[width=0.45\textwidth]{bristol_groundtruth}}
  \subfloat[]{
     \label{fig:inaccurate_gt}
     \includegraphics[width=0.45\textwidth]{inaccurate_groundtruth}}
     

  \caption[moving]{Original accurate ground truth on the left, inaccurate ground truth on the right}
  \label{fig:groundtruth}
}\end{figure}

\subsection{Experiment 1}

First experiment was done with minimal preprocessing; only the green timestamp was removed.
In unsupervised methods, the mask described in Section~\ref{subsec:vesselmask} was used to reduce the amount of false positives.
In supervised methods, features describing color values and the presence of edges were used.
The original value of red channel and the contrast enhanced values of green and blue channel were used to describe color.
For edge detection, local standard deviation of a 3-by-3 neighborhood of each channel was used.


% \subsection{Experiment 2}

% \section{DISCUSSION}
% \label{sec:discussion}

% We have to discuss what we learned.

% Notice the automatic page breaks. 

% \subsection{Segmentation Results}

% \subsection{Sensitivity of Image Features}

% \subsection{Future Work}

% Tehtiin laitokselle osana projektia, tulevaisuudessa saatu ground truth on
% todennäköisesti epätarkkaa ja piti tietää miten se vaikuttaa tuloksiin.
% It is always nice to give some ideas for the future.

% \section{CONCLUSIONS}
% \label{sec:conclusion}

% Finally the conclusions. This is more compact that the Discussion, a sort
% of summary about how things went on a general level.

% Now you can delete all this crap content and write your own. Have fun!


% \clearpage

% Bibliography
%
%% This must be here, not in preamble, if you want it to work
\addcontentsline{toc}{section}{REFERENCES}
\bibliography{resources/thesis}



%% ----------------------- APPENDICES ------------------------------
% \appendix


% \section{Appendix Guidelines}

% The appendices part starts with the command \verb:\appendix:. Then, each
% appendix must be started with \verb:\section{Appendix Name}: and ended
% with \verb:\sectionend: to have the continues/continued markings right.
% For example, see the multi-page appendices after this one.

% \sectionend


% \section{Frame Schematics}
% \label{app:frame}

% This is an appendix. If you need more appendices, just make a new section
% here (the \texttt{section} command).
% 
% \begin{figure}[htp]
%   {\par\centering
%   \includegraphics[width=0.95\textwidth]{exporesp}
%   \par}
%   \caption{Overall design, only one half drawn.}
%   \label{afig:frame1}
% \end{figure}
% 
% huhu
% 
% \begin{figure}[htp]
%   {\par\centering
%   \includegraphics[width=0.55\textwidth]{exporesp}
%   \par}
%   \caption{Another picture.}
%   \label{afig:frame2}
% \end{figure}
% 
% Reference testing: Figures~\ref{afig:frame1}, \ref{afig:frame2}, and
% \ref{afig:frame3}. Table~\ref{atab:test}.
% 
% \sectionend


% \section{The Second}
% 
% Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod
% tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,
% quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
% consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
% cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non
% proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
% 
% \begin{figure}[htp]
%   {\par\centering
%   \includegraphics[width=0.35\textwidth]{exporesp}
%   \par}
%   \caption{The same picture once more.}
%   \label{afig:frame3}
% \end{figure}
% 
% \begin{table}[hpt]
% \begin{center}
% \caption{Appendix test table.\label{atab:test}}
% \begin{tabular}{|l|r|l|}
% \hline
% minimum distance & 10 & px \\
% \hline
% \end{tabular}
% \end{center}
% \end{table}
% 
% \newpage
% 
% Aaand two more pages, to test the continues/continued marks.
% 
% 
% \newpage
% 
% Aaand one more page, to test the continues/continued marks.
% 
% \sectionend

\end{document}
