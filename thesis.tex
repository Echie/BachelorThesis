% $Name:  $
% $Id: thesis.tex,v 1.18 2010/10/08 13:39:40 paalanen Exp $


% The history of this template:
% - unknown, original version
% - Jarmo "trewas" Ilonen, masters thesis, 2003
% - Pekka "PQ" Paalanen, information processing project, 2004,
%     hints about graphicx and making PDF from Pasi Valminen
% - Pekka "PQ" Paalanen, Master's thesis, 2006
% - upgraded to pdflatex and 1.8.2010 thesis guidelines, 2010

% useful links:
% http://www.ctan.org/tex-archive/help/Catalogue/entries/grfguide.html
% http://www.tug.org/applications/hyperref/


\documentclass{lutmscthesis}[2010/09/22]
%\documentclass[draft]{lutmscthesis}   % leave figures blank, faster

\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english,finnish]{babel}

\usepackage{times}

\usepackage{setspace}
\usepackage{verbatim}
\usepackage[intlimits]{amsmath}

% Ensure figure captions are below and table captions are above the content.
\usepackage{float}
\floatstyle{plain}\restylefloat{figure}
\floatstyle{plaintop}\restylefloat{table}

\usepackage[pdfborder={0 0 0}]{hyperref}
%\usepackage[chapter]{algorithm}

%           Hyperref rationale - or just pain in the butt
%
% Load 'float' package first, because that will fix problems with 'algorithm'
% package interacting with hyperref.
%
% Hyperref must be the last package loaded, except...
% Load 'algorithm' AFTER hyperref, otherwise \theHalgorithm is
% undefined control sequence error appears.
%
% The TeXLive 2008 version of 'algorithmic' is buggy with hyperref.
% Use this bundled, special, hand-fixed version of algorithmic.sty
% instead. It is identified by version 2006/12/15.


\graphicspath{{resources/}}                % Graphics search path

\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\boldsymbol{#1}}
\newcommand{\diag}[1]{\mathrm{diag}(#1)}
\newcommand{\iprod}[1]{\left\langle #1 \right\rangle}
\newcommand{\me}{\mathrm{e}}
\newcommand{\mi}{\mathrm{i}}
\newcommand{\md}{\mathrm{d}}
\newcommand{\sse}{{}} %\mathrm{SSE}}
\newcommand{\trace}{\mathrm{Tr}\:}
\newcommand{\frp}[2]{{}^\mathrm{#1}\vect{#2}}
\newcommand{\frs}[3]{{}^\mathrm{#1}#2_\mathrm{#3}}
\newcommand{\frv}[3]{{}^\mathrm{#1}\vect{#2}_\mathrm{#3}}
\newcommand{\frm}[3]{{}^\mathrm{#1}\matr{#2}_\mathrm{#3}}
\newcommand{\colvec}[2]{\genfrac{[}{]}{0pt}{1}{#1}{#2}}
\newcommand{\relphantom}[1]{\mathrel{\phantom{#1}}}

% Thesis information
\title{Sensitivity of retinal image segmentation on ground truth accuracy}
\titlefin{Silmänpohjakuvien segmentoinnin herkkyys pohjamerkintöjen tarkkuudelle}
\author{Teemu Huovinen}

\Major{Degree Program in Computer Science}
\Majorfin{Tietotekniikan koulutusohjelma}

\Keywords{eye fundus, image segmentation, sensitivity analysis, ground truth}

\Keywordsfin{silmänpohja, segmentointi, herkkyysanalyysi, pohjamerkintä}

% For a single supervisor, \Supervisor{N.N.}
% For multiple supervisors, \Supervisors{N.N.\\ K.K.}, that is,
% use \\ to separate names.
% the same with \Examiner{} or \Examiners{}
\Supervisor{Lasse Lensu D.Sc. (Tech.)}
\Examiners{Lasse Lensu D.Sc. (Tech.)}

% The examiners for the Finnish abstract only.
\Tarkastajat{TkT Lasse Lensu}

% date of topic accepted in the council
\AcceptDate{January 1\textsuperscript{st}, 2014}
% date of signature
\SignDate{July 3\textsuperscript{rd}}
% Year in abstract pages
\Year{2014}

% Thesis statistics: figure, table and appendix counts, for abstracts
\addtostats{, 13 figures, 1 table, and 2 appendices}
\addtostatsfin{, 13 kuvaa, 1 taulukko ja 2 liitettä}

\begin{document}
\selectlanguage{english}

\maketitle
\newpage

\begin{abstract}
% Notice that the page, figure, table and appendix counts are for the
% whole work, including title page, appendices, figures in appendices, etc.
% You have to count the numbers yourself, except for pages.
% %
% Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod
% tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,
% quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
% consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
% cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non
% proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
\end{abstract}


\begin{tiivis}
% Tähän kirjoitetaan ytimekäs tiivistelmä: tausta, tavoite, tulokset ja johtopäätökset.
% Tiivistelmässä kannattaa käyttää lyhyen nasevia lauseita. Itse tekstissä voi käyttää
% monimutkaisempia lauseita. Tiivistelmä-sivu on yksi sivu ja tiivistelmäteksti on yksi
% kappale, ei useita kappaleita. On hyvä kertoa työn tavoitteet. Mikäli työ sisältää oleellisia
% aiheen rajauksia, ne kannattaa mainita jo tiivistelmässä. Työn tulokset ja johtopäätökset
% luetellaan lukijan mielenkiinnon lisäämiseksi. Opinnäytetyö kirjoitetaan passiivissa tyyliin
% ``tässä työssä tutkitaan..'' aktiivin sijaan ``minä tutkin...'' ja marginaalit tasataan aina sekä
% vasemmalle että oikealle. Työn numerointi aloitetaan kansilehdeltä, mutta tätä roomalaista
% numeroa ei merkitä näkyviin. 
\end{tiivis}


\begin{preface}

I would like to thank the Laboratory of Machine Vision and Pattern Recognition of Lappeenranta University of Technology
for giving me this opportunity to carry out my research. I would like to give special thanks to D.Sc. Lasse Lensu for 
the endless guidance and support during my summers as an intern.

I would like to thank University of Bristol for their retinal image database and its accurate ground truth data,
which was extensively used in this thesis.

\

Lappeenranta, October 19th, 2014
\end{preface}


% These name-definitions must be after Babel langugage change
% commands, as they redefine these.
\renewcommand\refname{REFERENCES}
\renewcommand\contentsname{CONTENTS}

\pagestyle{masters}
\newpage

\tableofcontents



\section*{ABBREVIATIONS AND SYMBOLS}

\begin{tabular}{l l}
\textbf{RGB} & Red Green Blue\\
\textbf{TP} & True Positive\\
\textbf{TN} & True Negative\\
\textbf{FP} & False Positive\\
\textbf{FN} & False Negative\\
\textbf{FPR} & False Positive Rate\\
\textbf{FNR} & False Negative Rate\\
\textbf{CLAHE} & Contrast Limited Adaptive Histogram Equalization\\
\textbf{GMM} & Gaussian Mixture Model\\
\textbf{pdf} & Probability Density Function\\
\end{tabular}


% space between paragraphs
\setlength{\parskip}{3ex}


\section{INTRODUCTION}

\subsection{Background}

%Diabetes has established itself as a seemingly permanent, worldwide global problem. 
The growing amount of diabetes patients and (arguably) more importantly the estimated amount of undiagnosed patients 
motivate the research for an effective mass screening method for monitoring and early detection of diabetes. The most common
complication of diabetes, diabetic retinopathy, causes abnormalities in the eye, and detecting these abnormalities in the eye fundus
is a promising mass screening method. ~\cite{Kauppi:2010} When developing a method for detecting these abnormalities, handmade annotations of the objects
in the image are used as a ground truth to train classifiers and to evaluate the results. In eye fundus image segmentation research,
ground truths are usually done by medical experts of the field (ophthalmologists) marking these abnormalities, such as exudates.

Optimal ground truth would be a pixel-accurate binary representation of the abnormalities, but as ground truths are
done by a human hand, such accuracy is not possible. Because the marking of an accurate ground truth takes a good
amount of time and patience, it is often necessary to have to settle for rough markings of the present abnormalities. Clusters of
exudates are circled, rather than each small finding specified separately.

\subsection{Objectives and Restrictions}

This thesis will address two main questions in its research:
\vspace{-5mm}
\begin{itemize} \itemsep1pt \parskip0pt
\parsep0pt
  \item How will inaccurate ground truth affect features and segmentation methods?
  \item How will other features than colour perform?
\end{itemize}
\vspace{-5mm}

% The objective of this thesis is to evaluate how big of an impact inaccurate ground truth has on various image features
% and segmentation methods. 

This thesis focuses on exudate detection in terms of segmentation performance, and Bristol database is used as it is 
readily available and has accurate ground truths of exudates. To enable comparison between features, colour, edge and
texture features are used. Blood vessel detection is explored only to create a mask for them. A rough method for optic
disk detection is also implemented as a preprocessing step for masking reasons. Structure of the eye fundus is briefly explored for context.

Both supervised and unsupervised segmentation methods are used. In supervised methods, ground truths are used to label
observations as either exudate or background. In unsupervised methods, ground truth is used to evaluate segmentation
results. Best parameters for each method are chosen based on their performance.

\subsection{Structure of the Thesis}

Section~\ref{sec:segmentation} takes a look at the different features
of eye fundus images, and how they are relevant in this thesis.
It also explains the theory behind the applied pre-processing
and segmentation methods.
Section~\ref{sec:sensitivity} details how sensitivity analysis is 
done in this thesis, and also explains the used evaluation methods.
Section~\ref{sec:experiments} describes the experiments in detail,
and presents the results for each experiment.
% Section~\ref{sec:discussion} sums up and interprets the results, and
% discusses the impact of this thesis and possible future work this thesis
% might invoke.

\section{RETINAL IMAGES AND THEIR SEGMENTATION}
\label{sec:segmentation}

\subsection{Structure of the eye fundus}
The function of the human eye on an abstract level can be compared to a camera. Light reflected from an object passes through the cornea,
pupil and lens and is focused on the retina (the inner part of the eye). It is then processed by photoreceptors and then transmitted to 
the brain via the optic nerve.~\cite{Wandell:1995} Rather than covering the eye as a whole, this chapter will focus on the structure of the eye fundus.

The eye fundus is the area located in the retina, at the very back of the eye. The location of the eye fundus is illustrated in
Figure~\ref{fig:cross_section_eye}. Its most noticeable parts are the optic disk, the macula, and the arteries. Macula is a
highly light sensitive area in the central region of the retina. Fovea is a round area located in the macula, which is densely
populated by cones (cells sensitive to color).~\cite{Kolb}

Optic disc is where the optic nerve and main arteries connect with the eye. There are no light sensitive cells inside the
optic disc, which creates a blind spot in the retina. Arteries inside the eye together with the choroid (vascular layer
surrounding the retina) provide nutritional support to the eye.~\cite{Kauppi:2010} These parts of the eye fundus
are illustrated in Figure~\ref{fig:healthyeyefundus}.

% \begin{figure}[ht]
% \centering 
% { \includegraphics[width=0.7\textwidth]{healthyeyefundus} }    
%   \caption[moving]{Structure of the eye fundus
%   \label{fig:healthyeyefundus}}
% \end{figure}


\begin{figure}[ht]
\centering
{
  \subfloat[]{
    \label{fig:cross_section_eye}
    \includegraphics[width=0.5\textwidth]{cross_section_eye}}
    \subfloat[]{
    \label{fig:healthyeyefundus}
    \includegraphics[width=0.5\textwidth]{healthyeyefundus}}
     
  \caption[moving]{Structure of the human eye
  \subref{fig:cross_section_eye} Cross section of the eye (modified from~\cite{Kolb})
  \subref{fig:healthyeyefundus} Structure of the fundus
  
  
  \label{fig:eyestructure}}
}\end{figure}



\subsection{Optic disc detection}
\label{subsec:opticdisc}
Optic disc is very similar to exudates in terms of color and intensity, so detection
and masking of the optic disk is an important preprocessing step in exudate detection.
There are papers dedicated to the localisation of the optic disk~\cite{Sekhar:2008},
and it is also covered in papers concerning the detection of other parts of the eye
fundus, such as exudates~\cite{Walter:2002}.

This method is based on the brightness of the optic disk, and the vertical blood vessels inside it.
The horizontal image gradient is calculated using Sobel operator, the result is shown in 
Figure~\ref{fig:opticdisk_gradient}. Image is then divided into slightly overlapping square 
areas with a side of 140 pixels (size is adjusted when operating close to image borders). The area
with the highest sum of gradients is considered as region of interest, i.e. to hold the optic disk.
This is because the dark blood vessels inside the bright optic disk result in a strong horizontal
gradient. Images with a ``camera glare``,i.e. a high intensity strip in the corner of the eye fundus
are problematic, as that area also has a high horizontal gradient. Region of interest is shown in Figure~\ref{fig:opticdisk_ROI}.

Inside this area of highest sum of horizontal gradients, the pixel with the highest intensity is considered
to be inside the optic disk. This pixel is then used as a center of a circle that will mask out the optic disk.
Final masking result is shown in Figure~\ref{fig:opticdisk_masked}.

\begin{figure}[ht]
\centering
{
  \subfloat[]{
     \label{fig:opticdisk_gradient}
     \includegraphics[width=0.45\textwidth]{OP_gradient}}
  \subfloat[]{
     \label{fig:opticdisk_ROI}
     \includegraphics[width=0.45\textwidth]{OP_ROI}}
     
     
  \subfloat[]{
     \label{fig:opticdisk_masked}
     \includegraphics[width=0.45\textwidth]{OP_masked}}

  \caption[moving]{Locating and masking the optic disk:
  \subref{fig:opticdisk_gradient} Horizontal gradient
  \subref{fig:opticdisk_ROI} Region of interest
  \subref{fig:opticdisk_masked} Optic disk masked out}
  \label{fig:opticdisk}
}\end{figure}


\subsection{Blood vessel detection}
\label{subsec:vesselmask}

The issue of blood vessel detection in fundus images has been a popular topic of research~\cite{Zana:1999}~\cite{Staal:2004}~\cite{Niemeijer:2004}. 
In the context of this thesis, however, the purpose of blood vessel detection is to create a mask, and to use that mask to remove false
positives from exudate segmentation results. For example, edge detection techniques often highlight the
borders of vessels as well as exudates. For this purpose, accurate vessel detection itself is not important and
including other dark areas of the image is even beneficial.

The mask is formed by first using CLAHE (contrast limited adaptive histogram equalization)~\cite{Reza:2004} to enhance
contrast in the green channel of the image, the result for this is shown in Figure~\ref{fig:vesselmask_adaptive}. This
contrast enhanced image is then thresholded with Otsu's method~\cite{Otsu:1975}, which separates the image into foreground
and background by minimizing the intra-class variance. This results in all the vessels and other darker areas showing as
black (or background), and all brighter areas as white (foreground). This is shown in Figure~\ref{fig:vesselmask_threshold}.
To create a binary mask of the darker areas, we use the complement of this thresholded image. Final version of the
mask is shown in Figure~\ref{fig:vesselmask_complement}.

This method is inadequate for blood vessel detection as it also includes other darker areas of the image, 
such as the fovea. As a mask however, it clearly reduces the amount of false positives in exudate segmentation results.
It also doesn't remove true positives, as only the darker areas of the image are included in the mask.

\begin{figure}[ht]
\centering
{
  \subfloat[]{
     \label{fig:vesselmask_adaptive}
     \includegraphics[width=0.45\textwidth]{vesselmask_adaptive}}
  \subfloat[]{
     \label{fig:vesselmask_threshold}
     \includegraphics[width=0.45\textwidth]{vesselmask_threshold}}
     
     
  \subfloat[]{
     \label{fig:vesselmask_complement}
     \includegraphics[width=0.45\textwidth]{vesselmask_complement}}

  \caption[moving]{Creating the blood vessel mask:
  \subref{fig:vesselmask_adaptive} CLAHE
  \subref{fig:vesselmask_threshold} Thresholding using Otsu's method
  \subref{fig:vesselmask_complement} Final mask, complement of thresholded image}
  \label{fig:vesselmask}
}\end{figure}

\subsection{Color transform}

The human eye is capable of correcting the effect of varying light sources (illuminants) in perception of color~\cite{Foster:2011}.
In contrast, an image of an object taken under different light sources is perceived differently in terms of color by a computer.
This is why color features need to be normalized in image sets where there's high variation in color properties.

The illumination and general color properties of the eye fundus images in Bristol database vary quite heavily.
To be able to effectively use color features in teaching a classifier, the variance between image needs to be minimized.
This is achieved by estimating the illuminant by applying the gray-world assumption~\cite{Buchsbaum:1980}:
\textit{the average reflectance in a scene under a neutral light source is achromatic.} Color feature normalization was 
then done by multiplying each channel with a coefficient defined as $ K_r = I_{avg} / r_{avg}$, where $I_{avg}$ is the mean
of all RGB-values in the image, and $r_{avg}$ is the mean of the specific channel. Results are shown in Figure~\ref{fig:colortransform}.

\begin{figure}[ht]
\centering
{
  \subfloat[]{
     \label{fig:original_010}
     \includegraphics[width=0.45\textwidth]{original_010}}     
  \subfloat[]{
     \label{fig:color_010}
     \includegraphics[width=0.45\textwidth]{color_010}}
     
  \subfloat[]{
     \label{fig:original_011}
     \includegraphics[width=0.45\textwidth]{original_011}}     
  \subfloat[]{
     \label{fig:color_011}
     \includegraphics[width=0.45\textwidth]{color_011}}

  \caption[moving]{Color space transform: Original images on the left, adjusted images on the right.}  
  \label{fig:colortransform}
}\end{figure}


\subsection{Unsupervised methods}

% An unsupervised method doesn't hold any information of the features it seeks, nor does it have any ''expectations'' of the image.
% It perform the same operations on each image with the object of clustering the data into meaningful sets.
% 
% Most edge detection methods are unsupervised algorithms. Edge is defined as a sharp change in image and color intensity, 
% and the purpose of an edge detection algorithm is to emphasize and detect those sharp changes in an image. In an eye fundus image,
% this would include the exudates, but also the borders of the blood vessels and optic disc. As such, edge detection isn't
% sufficient for exudate detection since these unwanted regions need to be subtracted from the initial result. During this study,
% this is accomplished with the mask acquired in Section~\ref{subsec:vesselmask}.

\subsubsection{Kirsch operator}

Edge detection operators that only detect gradients in specified directions are called compass operators.
Kirsch operator applies a compass operator in eight directions by rotating a mask in 45$^{\circ}$ shifts.~\cite{Jain:1989}
It is defined as:

\[
K_1 = \begin{bmatrix}
  5 & 5 & 5 \\
 -3 & 0 &-3 \\
 -3 &-3 &-3 
\end{bmatrix}
K_2 = \begin{bmatrix}
  5 & 5 &-3 \\
  5 & 0 &-3 \\
 -3 &-3 &-3 
\end{bmatrix} 
... K_8 = \begin{bmatrix}
 -3 & 5 & 5 \\
 -3 & 0 & 5 \\
 -3 &-3 &-3 
\end{bmatrix}

\]      
\vspace{-20mm}

\subsubsection{Morphological operations}

Mathematical morphology operations use a structuring element to perform an operation on an input image.
Most morphology operations are based the two basic morphological operations, dilation and erosion.
Top-hat transform is an operation that highlights bright areas in an image, where as bottom-hat
(also known as black top-hat) operation highlights the dim areas of an image.~\cite{Bai:2012}
These transforms are defined as follows:
\vspace{-2mm}
\begin{equation}\label{eq:tophat}
\begin{split}
TopHat(f) = f - (f \circ B)
\end{split}
\end{equation}
\vspace{-25mm}

\begin{equation}\label{eq:bottomhat}
\begin{split}
BottomHat(f) = (f \bullet B) - f
\end{split}
\end{equation}
\vspace{-12mm}


where $f$ is the input image, $B$ is the structuring element, $\circ$ denotes the opening operation and $\bullet$
denotes the closing operation. Opening and closing are operations based on dilation and erosion, and they are defined in~\cite{Bai:2012}.

The exudate detection method proposed in~\cite{Eadgahi:2012} was also implemented, except for the optic disc detection, for which 
the method described in~\ref{subsec:opticdisc} was used. The method is designed to use top-hat operation to highlight the exudates, 
and bottom-hat operation to highlight and ultimately remove dim areas, e.g vessels from the results. The method is defined as follows:

\vspace{-12mm}
\begin{equation}\label{eq:mehdi}
\begin{split}
F(f) = TopHat(f) - BottomHat(f)
\end{split}
\end{equation}
\vspace{-10mm}

where $Tophat(f)$ and $BottomHat(f)$ denote the Equations \ref{eq:tophat} and \ref{eq:bottomhat}, respectively.
During the computations performed for this thesis, thresholding is applied after these operations even though
it is not mentioned in~\cite{Eadgahi:2012}.

\subsection{Supervised methods}

% Before a supervised method (or its classifier) can be used in classification, it needs to be trained. A specific training set
% of labeled images is used to teach the classifier. The method then extracts features from the training images, and generally forms a model
% for each used label. For example, to describe a fruit you might use its color and shape. You ''teach'' the method what each
% fruit looks like by labeling a set of training images. The classifier learns which features describe each label. Yellow and round equals orange,
% yellow and long equals banana, green and round equals apple. After training, the classifier can be used to classify new images.

\subsubsection{Gaussian Mixture Models}

A Gaussian Mixture Model (GMM) is a single probability density function (pdf) that describes a weighted sum of multiple Gaussian pdfs~\cite{Reynolds:2009}.
When using a GMM to model image classes (more specifically, class conditional pdfs), parameters of the underlying Gaussian pdfs are estimated by fitting the
model to training data. After the formation of the model, it is possible to use Bayesian classification to classify pixels to the given classes.

Application, parameter estimation and performance of a classifier using GMMs and Bayesian classification is discussed in~\cite{paalanen2006feature}.
GMMBayes Toolbox~\cite{Paalanen:2004} is a MATLAB implementation of said methods. For parameter estimation, Expectation Maximization (EM), Figuero-Jain (FJ)
and greedy EM algorithms are implemented in the Toolbox and described in both~\cite{paalanen2006feature} and~\cite{Paalanen:2004}. The main difference between
these algorithms is that EM requires a fixed amount of Gaussian components, while FJ estimates the amount (maximum amount is fixed in the implementation).
In this thesis, only Figuero-Jain is used.

\subsubsection{Naive Bayes}

Naive Bayes is a probabilistic classifier that gets its name from the assumption that all features are independent from each other. Each feature
contributes to the classification regardless of the absence or presence of other features. As the name also implies, the Naive Bayes classifier
uses the Bayes' theorem as a basis of classification.~\cite{Murphy:2006}
-
The Naive Bayes classifier calculates the posterior probability for the sample to belong in each of the possible classes based on its features, and then
classifies the sample to the class with the highest posterior probability. The classes are modeled with a single probability density function, and in the case of
binary pixel classification, classes are foreground and background. In this thesis, exudates represent the foreground and the rest 
is considered background. The posterior probability is calculated with the Bayes' theorem as follows:

\begin{equation}\label{eq:naivebayes}
\begin{split}
p(C_1) = \frac{P(C_1)p(\overline{x}|C_1)}{P(C_1)p(\overline{x}|C_1) + P(C_2)p(\overline{x}|C_2)}
\end{split}
\end{equation}

where $ p(C_1) $ is the posterior probability and $ P(C_1) $ is the prior probability for the sample to belong in class one, and $ p(\overline{x}|C_1) $
denotes the conditional probability of the feature vector having the values of the sample pixel, if it were of class one.

\section{SENSITIVITY ANALYSIS OF IMAGE FEATURES}
\label{sec:sensitivity}

\subsection{Evaluation methods}

In evaluation, correctly classified samples can be split into true positives and true negatives. Errors are then similarly split
into false positives and false negatives. In this thesis, true positives are findings correctly classified as exudates 
in the segmentation results, and true negatives are consequently findings correctly classified as background. False
negatives are classified as background in results, but are actually positive (classified as exudate) in ground truth, and
false positives are classified as positive in results, when they are in fact background~\cite{Fawcett:2006}. These terms are illustrated with
a error matrix shown in Table ~\ref{tab:truepositive}.

\begin{table}[hpt]
\begin{center}
\caption{General error matrix\label{tab:truepositive}}

\begin{tabular}{cc|c|c|c|c|l}
\cline{3-4}
& & \multicolumn{2}{ c| }{Ground truth} \\ \cline{3-4}
& & Positive & Negative  \\ \cline{1-4}
\multicolumn{1}{ |c| }{\multirow{}{}{Tests} } &
\multicolumn{1}{ |c| }{Positive} & True Positive (TP)& False Positive (FP) \\ \cline{2-4}
\multicolumn{1}{ |c  }{}  &
\multicolumn{1}{ |c| }{Negative} & False Negative (FN)& True Negative (TN) \\ \cline{1-4}

\end{tabular}
\end{center}
\end{table}


There exists a multitude of ways to evaluate segmentation results. In this thesis, it was desired to have a single number
describing the ''goodness'' of segmentation results to enable easier comparison and ranking of results. Different values,
such as sensitivity, specificity and precision (defined in~\cite{Fawcett:2006}) were considered, but these were ignored as they
only described one aspect of the result, either the amount of samples correctly positive or negative. The used coefficient
would have to describe the ''goodness'' in a more wholesome way, and for that reason, the Dice coefficient~\cite{Dice:1945}, Jaccard index    
and F-score were considered. These coefficients are defined as follows:

\begin{equation}\label{eq:dice}
\begin{split}
Dice = \frac{2|A\cap B|}{|A| + |B|}
\end{split}
\end{equation}
\vspace{-2mm}
\begin{equation}\label{eq:jaccard}
\begin{split}
Jaccard = \frac{|A\cap B|}{|A\cup B|}
\end{split}
\end{equation}
\vspace{-2mm}
\begin{equation}\label{eq:fscore}
\begin{split}
F-score = \frac{2PR}{P + R}
\end{split}
\end{equation}


where A is the segmented set, and B is the ground truth, P stands for precision and R stands for recall (sensitivity).
%Dice coefficient can also be described using true positives, true negatives, false positives and false negatives:

%\begin{equation}\label{eq:diceTn}
%\begin{split}
%Dice = \frac{2TP}{(FP+TP)+(TP +FN)}
%\end{split}
%\end{equation}

Dice coefficient and F-score values are interchangeable in practice as their values are exactly equal, and Jaccard index behaves 
similary to the two. Dice coefficient was chosen as it is very simple to implement and understand. It can have values from the
range [0,1], where 0 indicates no similarities, and 1 indicates perfect agreement.



\subsection{Effects of inaccurate ground truth}

When researching feature sensitivity to ground truth accuracy, standard segmentation with unsupervised methods isn't really useful as
it doesn't use the ground truth. In this thesis, the ground truth is used in finding the best parameters for unsupervised methods.
First, the images used for teaching are segmented with each method and a large set of parameters. The results are then evaluated with
the ground truth. Parameters with the highest Dice (see Eq.~\ref{eq:dice}) coefficient are then selected to be used in experiments.

The accuracy of ground truth has a direct impact on training and performance of supervised methods. In general, inaccurate ground truth 
will mean a significant amount of background samples close to exudates will be categorized as positive. This will result in higher 
variation of feature distributions and an increase of false positives.

\section{EXPERIMENTS AND RESULTS}
\label{sec:experiments}
% \subsection{Retinal Image Databases}

\subsection{The Ground Truth}

The ground truth of exudates in Bristol database is very accurate, and to enable comparison of results and sensitivity analysis,
more inaccurate ground truths were made by hand. Instead of the original color images, markings for the inaccurate ground truth were
made on the black-and-white images of Bristol ground truth. This was to ensure every exudate present in the Bristol ground truth was
also present in the inaccurate ground truth. Markings were done in a way that estimated the way doctors marked their findings 
when given the freedom to make inaccurate markings. Essentially this means that clusters of exudates are grouped together,
and single exudates were more loosely circled. This is illustrated in Figure~\ref{fig:groundtruth}.

To create a basis for more comprehensive testing, different stages of inaccurate ground truths were created by dilating the accurate
ground truth. Three different stages were created; one, three and five iterations of dilation by a disk-shaped structuring element, with a radius of 1 pixel.
The dilation was restricted to stay inside the inaccurate ground truth.

\begin{figure}[ht]
\centering
{
  \subfloat[]{
     \label{fig:bristol_gt}
     \includegraphics[width=0.45\textwidth]{bristol_groundtruth}}
  \subfloat[]{
     \label{fig:inaccurate_gt}
     \includegraphics[width=0.45\textwidth]{inaccurate_groundtruth}}
     
  \caption[moving]{Ground truths:
  \subref{fig:bristol_gt} Original accurate ground truth
  \subref{fig:inaccurate_gt} Inaccurate ground truth }
  \label{fig:groundtruth}

}\end{figure}


% \subsection{Experiment 1}
% 
% First experiment was done with minimal preprocessing; only the green timestamp was removed.
% In unsupervised methods, the mask described in Section~\ref{subsec:vesselmask} was used to reduce the amount of false positives.
% In supervised methods, features describing color values and the presence of edges were used.
% The original value of red channel and the contrast enhanced values of green and blue channel were used to describe color.
% For edge detection, local standard deviation of a 3-by-3 neighborhood of each channel was used.


% \subsection{Experiment 2}

% \section{DISCUSSION}
% \label{sec:discussion}

% We have to discuss what we learned.

% Notice the automatic page breaks. 

% \subsection{Segmentation Results}

% \subsection{Sensitivity of Image Features}

% \subsection{Future Work}

% Tehtiin laitokselle osana projektia, tulevaisuudessa saatu ground truth on
% todennäköisesti epätarkkaa ja piti tietää miten se vaikuttaa tuloksiin.
% It is always nice to give some ideas for the future.

% \section{CONCLUSIONS}
% \label{sec:conclusion}

% Finally the conclusions. This is more compact that the Discussion, a sort
% of summary about how things went on a general level.

% Now you can delete all this crap content and write your own. Have fun!


% \clearpage

% Bibliography
%
%% This must be here, not in preamble, if you want it to work
\addcontentsline{toc}{section}{REFERENCES}
\bibliography{resources/thesis}



%% ----------------------- APPENDICES ------------------------------
% \appendix


% \section{Appendix Guidelines}

% The appendices part starts with the command \verb:\appendix:. Then, each
% appendix must be started with \verb:\section{Appendix Name}: and ended
% with \verb:\sectionend: to have the continues/continued markings right.
% For example, see the multi-page appendices after this one.

% \sectionend


% \section{Frame Schematics}
% \label{app:frame}

% This is an appendix. If you need more appendices, just make a new section
% here (the \texttt{section} command).
% 
% \begin{figure}[htp]
%   {\par\centering
%   \includegraphics[width=0.95\textwidth]{exporesp}
%   \par}
%   \caption{Overall design, only one half drawn.}
%   \label{afig:frame1}
% \end{figure}
% 
% huhu
% 
% \begin{figure}[htp]
%   {\par\centering
%   \includegraphics[width=0.55\textwidth]{exporesp}
%   \par}
%   \caption{Another picture.}
%   \label{afig:frame2}
% \end{figure}
% 
% Reference testing: Figures~\ref{afig:frame1}, \ref{afig:frame2}, and
% \ref{afig:frame3}. Table~\ref{atab:test}.
% 
% \sectionend


% \section{The Second}
% 
% Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod
% tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,
% quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
% consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse
% cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non
% proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
% 
% \begin{figure}[htp]
%   {\par\centering
%   \includegraphics[width=0.35\textwidth]{exporesp}
%   \par}
%   \caption{The same picture once more.}
%   \label{afig:frame3}
% \end{figure}
% 
% \begin{table}[hpt]
% \begin{center}
% \caption{Appendix test table.\label{atab:test}}
% \begin{tabular}{|l|r|l|}
% \hline
% minimum distance & 10 & px \\
% \hline
% \end{tabular}
% \end{center}
% \end{table}
% 
% \newpage
% 
% Aaand two more pages, to test the continues/continued marks.
% 
% 
% \newpage
% 
% Aaand one more page, to test the continues/continued marks.
% 
% \sectionend

\end{document}
